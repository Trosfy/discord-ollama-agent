# Docker Compose - Application Services
#
# Toggleable application services that can be started/stopped via admin dashboard.
# Infrastructure services (admin, auth, web, logging, dynamodb) remain running.
#
# Services:
# - fastapi-service: Main LLM API and orchestrator (legacy profile)
# - troise-ai: New TROISE AI service
# - discord-bot: Discord bot client
#
# SGLang is managed separately via: ./scripts/model_management/sglang/
#
# Usage:
#   Start apps: docker compose -f docker-compose.app.yml up -d
#   Stop apps: docker compose -f docker-compose.app.yml down
#   View app logs: docker compose -f docker-compose.app.yml logs -f

services:
  fastapi-service:
    profiles: ["legacy"]  # Use --profile legacy to start old fastapi-service
    build: ./fastapi-service
    container_name: trollama-fastapi
    ports:
      - "8001:8000"
    extra_hosts:
      - "host.docker.internal:host-gateway"  # Enable host.docker.internal on Linux
    env_file:
      - .env
    environment:
      - DYNAMODB_ENDPOINT=http://trollama-dynamodb:8000
      - LOGGING_HOST=trollama-logging
      - LOGGING_PORT=9999
      - PYTHONPATH=/shared
      - SGLANG_ENDPOINT=http://trollama-sglang:30000
    volumes:
      - ./fastapi-service:/app
      - ./shared:/shared
      - temp-files:/tmp/discord-bot-artifacts
      - temp-files:/tmp/discord-bot-uploads
    # NOTE: Infrastructure services (dynamodb, logging) must be running before starting
    # App services connect to them via container names on trollama-network
    restart: unless-stopped
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  troise-ai:
    build: ./troise-ai
    container_name: troise-ai
    ports:
      - "8001:8000"  # Same port as fastapi-service (replaces it)
    extra_hosts:
      - "host.docker.internal:host-gateway"  # Enable host.docker.internal on Linux
    env_file:
      - .env
    environment:
      - DYNAMODB_ENDPOINT=http://trollama-dynamodb:8000
      - LOGGING_HOST=trollama-logging
      - LOGGING_PORT=9999
      - PYTHONPATH=/shared
      - SGLANG_ENDPOINT=http://trollama-sglang:30000
      # MinIO settings
      - MINIO_ENDPOINT=troise-minio:9000
      - MINIO_ACCESS_KEY=minioadmin
      - MINIO_SECRET_KEY=minioadmin
      - MINIO_BUCKET=troise-files
      # SearXNG for web search
      - SEARXNG_HOST=http://troise-searxng:8080
      # WebSocket Authentication
      - JWT_SECRET_KEY=${JWT_SECRET_KEY}
      - BOT_SECRET=${BOT_SECRET:-}
      - DISABLE_WS_AUTH=${DISABLE_WS_AUTH:-false}
      # HuggingFace cache location inside container
      - HF_HOME=/home/app/.cache/huggingface
      # ComfyUI for NVFP4 image generation
      - COMFYUI_HOST=http://troise-comfyui:8188
    volumes:
      - ./troise-ai:/app
      - ./shared:/shared
      - ./troise-ai/config:/app/config
      # Mount HuggingFace cache for FLUX model
      - /home/trosfy/.cache/huggingface:/home/app/.cache/huggingface
    # GPU access for FLUX image generation
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    # Wait for ComfyUI to be healthy before starting warmup
    depends_on:
      comfyui:
        condition: service_healthy
    restart: unless-stopped
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  discord-bot:
    build: ./discord-bot
    container_name: trollama-discord-bot
    env_file:
      - .env
    environment:
      - TROISE_WS_URL=ws://troise-ai:8000  # TROISE AI WebSocket (native protocol)
      - ADMIN_SERVICE_URL=http://trollama-admin:8000
      - LOGGING_HOST=trollama-logging
      - LOGGING_PORT=9999
      - PYTHONPATH=/shared
      # WebSocket Authentication (HMAC signature)
      - BOT_SECRET=${BOT_SECRET:-}
      # MinIO (for fetching generated images)
      - MINIO_ENDPOINT=troise-minio:9000
      - MINIO_ACCESS_KEY=${MINIO_ACCESS_KEY:-minioadmin}
      - MINIO_SECRET_KEY=${MINIO_SECRET_KEY:-minioadmin}
      - MINIO_BUCKET=troise-files
    ports:
      - "9997:9998"  # Health check port (external:internal)
    volumes:
      - ./discord-bot:/app
      - ./shared:/shared
    depends_on:
      troise-ai:
        condition: service_healthy
    restart: unless-stopped
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    # Health endpoint checks WebSocket connection status

  # SearXNG - Privacy-focused metasearch engine (used by troise-ai web search)
  searxng:
    image: searxng/searxng:latest
    container_name: troise-searxng
    restart: unless-stopped
    ports:
      - "8082:8080"  # External 8082 (8080 used by trollama-web), internal 8080
    volumes:
      - ./troise-ai/docker/searxng:/etc/searxng:rw
    environment:
      - SEARXNG_BASE_URL=http://localhost:8080/
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:8080/"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # Browserless - Headless Chrome for web scraping (used by troise-ai)
  browserless:
    image: browserless/chrome:latest
    container_name: troise-browserless
    restart: unless-stopped
    ports:
      - "3000:3000"
    environment:
      - MAX_CONCURRENT_SESSIONS=5
      - MAX_QUEUE_LENGTH=10
      - CONNECTION_TIMEOUT=60000
      - TIMEOUT=60000
      - PREBOOT_CHROME=true
      - KEEP_ALIVE=true
      - ENABLE_DEBUGGER=false
      - ENABLE_CORS=true
      - DEFAULT_STEALTH=true
      - DEFAULT_BLOCK_ADS=true
    deploy:
      resources:
        limits:
          memory: 2G
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:3000/"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 15s
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # MinIO - S3-compatible object storage for file uploads
  minio:
    image: minio/minio
    container_name: troise-minio
    restart: unless-stopped
    ports:
      - "9000:9000"
      - "9001:9001"
    environment:
      - MINIO_ROOT_USER=minioadmin
      - MINIO_ROOT_PASSWORD=minioadmin
    command: server /data --console-address ":9001"
    volumes:
      - minio_data:/data
    healthcheck:
      test: timeout 5s bash -c ':> /dev/tcp/127.0.0.1/9000' || exit 1
      interval: 5s
      timeout: 10s
      retries: 5
      start_period: 10s
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # ComfyUI - Node-based GenAI with native NVFP4 support for FLUX image generation
  # Custom ARM64 build for DGX Spark (Blackwell)
  comfyui:
    build:
      context: ./comfyui
      dockerfile: Dockerfile
    image: troise-comfyui:arm64
    container_name: troise-comfyui
    restart: unless-stopped
    ports:
      - "8188:8188"  # ComfyUI web UI and API
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - HF_HOME=/root/.cache/huggingface
    volumes:
      # Mount HuggingFace cache
      - /home/trosfy/.cache/huggingface:/root/.cache/huggingface
      # Mount output directory for generated images
      - comfyui_output:/opt/ComfyUI/output
      # Mount FLUX2 NVFP4 diffusion model
      - /home/trosfy/.cache/huggingface/hub/models--black-forest-labs--FLUX.2-dev-NVFP4/snapshots/142b87e70bc3006937b7093d89ff287b5f59f071/flux2-dev-nvfp4.safetensors:/opt/ComfyUI/models/diffusion_models/flux2-dev-nvfp4.safetensors:ro
      # Mount FLUX2 text encoder (FP4 mixed)
      - /home/trosfy/.cache/huggingface/comfyui-flux2/mistral_3_small_flux2_fp4_mixed.safetensors:/opt/ComfyUI/models/text_encoders/mistral_3_small_flux2_fp4_mixed.safetensors:ro
      # Mount FLUX2 VAE
      - /home/trosfy/.cache/huggingface/comfyui-flux2/flux2-vae.safetensors:/opt/ComfyUI/models/vae/flux2-vae.safetensors:ro
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8188/"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

volumes:
  temp-files:
    driver: local
  minio_data:
    driver: local
  comfyui_output:
    driver: local

networks:
  default:
    name: trollama-network
