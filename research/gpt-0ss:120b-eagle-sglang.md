# GPT-OSS-120B Eagle3 Implementation - SGLang Backend Integration

**Target Platform**: NVIDIA DGX Spark (Grace Blackwell GB10)  
**Model**: openai/gpt-oss-120b + EAGLE3 speculative decoding  
**Backend**: SGLang (NOT TensorRT-LLM due to SM 12.1 infinite loop bug)

---

## Architecture Overview

```
AWS Strands (orchestration, tools, routing)
    â†“
CompositeBackendManager (strategy pattern)
    â†“
â”œâ”€ OllamaBackendManager â†’ Ollama API (port 11434) [EXISTING]
â”œâ”€ SGLangBackendManager â†’ SGLang API (port 30000) [NEW]
â””â”€ [TensorRT/vLLM stubs remain for future use]

SGLang Server (separate container/process)
    â†“ 
gpt-oss-120b (MXFP4 quantization)
    + EAGLE3 speculative decoding
    = 55-70 tok/s on GB10
```

**Why SGLang, Not TensorRT-LLM**:
- TensorRT-LLM 1.2.0+ has **confirmed infinite loop bug** on Blackwell SM 12.1 (Issue #8615)
- Bug affects RTX Pro 6000 (same architecture as GB10)
- NVIDIA recommends TensorRT-LLM 1.1.0rc0, but only documented for GB200/B200 (not GB10)
- SGLang uses PyTorch backend - no SM-specific kernel issues

---

## Phase 1: SGLang Backend Manager

**File**: `fastapi-service/app/implementations/sglang_backend.py`

```python
"""
SGLang Backend Manager - OpenAI-compatible API integration.

Integrates with AWS Strands orchestration as a swappable backend.
"""

import asyncio
import httpx
from typing import AsyncGenerator, Dict, Optional, Set
from datetime import datetime
from circuitbreaker import circuit

from app.interfaces.backend_interface import IBackendManager
from app.config import BackendConfig, BackendType, settings
from app.utils.logging import get_logger

logger = get_logger(__name__)


class SGLangBackendManager(IBackendManager):
    """
    Backend manager for SGLang server.
    
    SGLang provides OpenAI-compatible API endpoints:
    - /v1/completions (non-streaming, streaming)
    - /v1/chat/completions
    - /v1/models
    
    Key differences from Ollama:
    - Models loaded at server startup (not per-request)
    - No dynamic model loading/unloading
    - Restart server to switch models
    - EAGLE3 configured at server launch
    """
    
    def __init__(self, base_url: str = "http://localhost:30000"):
        self.base_url = base_url
        self.client = httpx.AsyncClient(
            timeout=httpx.Timeout(120.0, connect=10.0)
        )
        self._circuit_breaker_failures = 0
        self._circuit_breaker_open = False
    
    def supports(self, backend_type: BackendType) -> bool:
        return backend_type == BackendType.SGLANG
    
    async def load(
        self, 
        model_id: str, 
        backend_config: BackendConfig
    ) -> None:
        """
        Verify model is loaded in SGLang server.
        
        SGLang loads models at startup, so this is verification only.
        Raises ValueError if model not available.
        """
        try:
            response = await self.client.get(f"{self.base_url}/v1/models")
            response.raise_for_status()
            models_data = response.json()
            
            available_models = [m["id"] for m in models_data.get("data", [])]
            
            if model_id not in available_models:
                raise ValueError(
                    f"Model '{model_id}' not loaded in SGLang server.\n"
                    f"Available models: {available_models}\n"
                    f"Restart SGLang server with --model-path {model_id}"
                )
            
            logger.info(f"âœ… Verified {model_id} is loaded in SGLang server")
            
        except httpx.HTTPError as e:
            raise RuntimeError(
                f"Failed to connect to SGLang server at {self.base_url}: {e}\n"
                f"Is SGLang server running?"
            )
    
    async def unload(
        self, 
        model_id: str, 
        backend_type: BackendType
    ) -> None:
        """
        SGLang doesn't support dynamic unloading.
        
        This is a no-op. To switch models, restart SGLang server.
        """
        logger.warning(
            f"SGLang doesn't support dynamic unloading of {model_id}. "
            f"To switch models, restart SGLang server with different --model-path"
        )
    
    @circuit(failure_threshold=5, recovery_timeout=60, expected_exception=Exception)
    async def generate(
        self,
        model_id: str,
        prompt: str,
        temperature: float = 0.7,
        max_tokens: int = 2048,
        **kwargs
    ) -> str:
        """
        Non-streaming generation via SGLang.
        
        Circuit breaker protects against cascading failures.
        """
        try:
            response = await self.client.post(
                f"{self.base_url}/v1/completions",
                json={
                    "model": model_id,
                    "prompt": prompt,
                    "temperature": temperature,
                    "max_tokens": max_tokens,
                    "stream": False,
                    **kwargs
                }
            )
            
            response.raise_for_status()
            result = response.json()
            
            return result["choices"][0]["text"]
            
        except Exception as e:
            logger.error(f"SGLang generation failed: {e}")
            raise
    
    @circuit(failure_threshold=5, recovery_timeout=60, expected_exception=Exception)
    async def generate_stream(
        self,
        model_id: str,
        prompt: str,
        temperature: float = 0.7,
        max_tokens: int = 2048,
        **kwargs
    ) -> AsyncGenerator[str, None]:
        """
        Streaming generation via SGLang.
        
        Yields text chunks as they're generated.
        Circuit breaker protects against cascading failures.
        """
        try:
            async with self.client.stream(
                "POST",
                f"{self.base_url}/v1/completions",
                json={
                    "model": model_id,
                    "prompt": prompt,
                    "temperature": temperature,
                    "max_tokens": max_tokens,
                    "stream": True,
                    **kwargs
                },
                timeout=120.0
            ) as response:
                response.raise_for_status()
                
                async for line in response.aiter_lines():
                    if not line.strip():
                        continue
                    
                    if line.startswith("data: "):
                        data = line[6:]  # Remove "data: " prefix
                        
                        if data.strip() == "[DONE]":
                            break
                        
                        try:
                            import json
                            chunk = json.loads(data)
                            
                            if "choices" in chunk and len(chunk["choices"]) > 0:
                                text = chunk["choices"][0].get("text", "")
                                if text:
                                    yield text
                        except json.JSONDecodeError:
                            logger.warning(f"Failed to parse SSE chunk: {data}")
                            continue
        
        except Exception as e:
            logger.error(f"SGLang streaming failed: {e}")
            raise
    
    def get_loaded_models(self) -> Set[str]:
        """
        Query SGLang server for loaded models.
        
        Used by VRAM orchestrator for registry reconciliation.
        """
        try:
            response = httpx.get(
                f"{self.base_url}/v1/models",
                timeout=5.0
            )
            response.raise_for_status()
            models_data = response.json()
            
            return {m["id"] for m in models_data.get("data", [])}
            
        except Exception as e:
            logger.warning(f"Failed to query SGLang models: {e}")
            return set()
    
    async def health_check(self) -> bool:
        """Check if SGLang server is responsive."""
        try:
            response = await self.client.get(
                f"{self.base_url}/health",
                timeout=5.0
            )
            return response.status_code == 200
        except Exception:
            return False


# Factory function for dependency injection
def create_sglang_backend(base_url: Optional[str] = None) -> SGLangBackendManager:
    """Create SGLang backend manager with configuration."""
    url = base_url or settings.SGLANG_ENDPOINT
    return SGLangBackendManager(base_url=url)
```

---

## Phase 2: Update Backend Types & Composite Manager

**File**: `fastapi-service/app/config/__init__.py`

```python
# Add SGLANG to BackendType enum
class BackendType(str, Enum):
    OLLAMA = "ollama"
    SGLANG = "sglang"      # â† NEW
    TENSORRT = "tensorrt-llm"
    VLLM = "vllm"


# Add SGLang settings
class Settings(BaseSettings):
    # ... existing settings ...
    
    # SGLang Backend
    SGLANG_ENDPOINT: str = "http://localhost:30000"  # or http://sglang-server:30000 in Docker
```

**File**: `fastapi-service/app/services/vram/backend_managers.py`

```python
# Update CompositeBackendManager

from app.implementations.sglang_backend import SGLangBackendManager

class CompositeBackendManager(IBackendManager):
    """
    Composite manager that delegates to appropriate backend.
    
    Updated to include SGLang backend for Eagle3 models.
    """
    
    def __init__(self):
        self._managers = [
            OllamaBackendManager(),
            SGLangBackendManager(),      # â† ADD THIS
            TensorRTBackendManager(),     # Keep for future
            vLLMBackendManager()          # Keep for future
        ]
```

---

## Phase 3: Update Model Configuration

**File**: `fastapi-service/app/config/profiles/performance.py`

```python
from app.config import ModelCapabilities, BackendConfig

class PerformanceProfile:
    """Configuration for 128GB VRAM systems optimized for maximum speed (DGX Spark GB10)."""

    @property
    def available_models(self) -> List[ModelCapabilities]:
        return [
            # Primary model: gpt-oss-120b-eagle3 for ALL text tasks (SGLang backend)
            ModelCapabilities(
                name="gpt-oss-120b-eagle3",
                backend=BackendConfig(
                    type="sglang",
                    endpoint="http://sglang-server:30000"  # Docker network
                ),
                vram_size_gb=84.0,  # Actual measured: 67GB base + 3GB KV cache + 14GB overhead
                priority="CRITICAL",  # Never evict - this is our only text model
                supports_tools=True,  # Base model supports function calling
                supports_thinking=False,  # Eagle3 focused on throughput
                context_window=40960
            ),

            # Specialized models (Ollama only)
            ModelCapabilities(
                name="ministral-3:14b",
                backend=BackendConfig(type="ollama", options={"keep_alive": "30m"}),
                supports_vision=True,
                supports_tools=True,
                vram_size_gb=9.1,
                priority="HIGH"  # Vision tasks need this
            ),

            ModelCapabilities(
                name="qwen3-embedding:4b",
                backend=BackendConfig(type="ollama", options={"keep_alive": "60m"}),
                supports_tools=False,
                vram_size_gb=2.5,
                priority="HIGH"  # Embeddings/RAG need this
            ),
        ]

    @property
    def vram_soft_limit_gb(self) -> float:
        return 10.0  # 119GB - 84GB SGLang - 25GB buffer

    @property
    def vram_hard_limit_gb(self) -> float:
        return 12.0  # 119GB - 84GB SGLang - 23GB buffer

    # All text routes use gpt-oss-120b-eagle3
    @property
    def router_model(self) -> str:
        return "gpt-oss-120b-eagle3"

    @property
    def simple_coder_model(self) -> str:
        return "gpt-oss-120b-eagle3"

    @property
    def complex_coder_model(self) -> str:
        return "gpt-oss-120b-eagle3"

    @property
    def reasoning_model(self) -> str:
        return "gpt-oss-120b-eagle3"

    @property
    def research_model(self) -> str:
        return "gpt-oss-120b-eagle3"

    @property
    def math_model(self) -> str:
        return "gpt-oss-120b-eagle3"
```

**IMPORTANT**: Updated memory estimate from 90GB to **84GB** based on actual measurements with 40K context.

---

## Phase 4: Update Strands LLM Implementation

**File**: `fastapi-service/app/implementations/strands_llm.py`

```python
# Update generate_stream_with_route to route to correct backend

async def generate_stream_with_route(
    self,
    prompt: str,
    model_id: str,
    temperature: float = 0.7,
    route: str = "REASONING",
    thinking_enabled: bool = False,
    **kwargs
) -> AsyncGenerator[str, None]:
    """
    Generate streaming response with backend routing.
    
    Routes to SGLang for Eagle3 models, Ollama for others.
    """
    
    # Get model capabilities
    model_caps = settings.get_model_capabilities(model_id)
    backend_config = model_caps.backend
    
    # Get appropriate backend manager
    backend_manager = self._get_backend_manager(backend_config)
    
    # VRAM orchestrator: Request model load
    orchestrator = get_orchestrator()
    if orchestrator:
        await orchestrator.request_model_load(
            model_id=model_id,
            backend=backend_config.type,
            required_gb=model_caps.vram_size_gb,
            priority=model_caps.priority
        )
    
    # Generate via backend
    try:
        async for chunk in backend_manager.generate_stream(
            model_id=model_id,
            prompt=prompt,
            temperature=temperature,
            **kwargs
        ):
            yield chunk
    
    except Exception as e:
        logger.error(f"Generation failed for {model_id}: {e}")
        
        # Mark as crashed for circuit breaker
        if orchestrator:
            await orchestrator.mark_model_unloaded(
                model_id=model_id,
                crashed=True
            )
        
        raise


def _get_backend_manager(self, backend_config: BackendConfig):
    """Get appropriate backend manager based on config."""
    
    if backend_config.type == "sglang":
        from app.implementations.sglang_backend import create_sglang_backend
        return create_sglang_backend(backend_config.endpoint)
    
    elif backend_config.type == "ollama":
        from app.implementations.ollama_backend import OllamaBackend
        return OllamaBackend()
    
    else:
        raise ValueError(f"Unsupported backend type: {backend_config.type}")
```

---

## Phase 5: Docker Deployment

**File**: `docker-compose.yml`

```yaml
services:
  # NEW: SGLang Server for gpt-oss-120b Eagle3
  sglang-server:
    image: lmsysorg/sglang:spark  # GB10-compatible image
    container_name: trollama-sglang
    ports:
      - "30000:30000"
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
      - /models:/models:ro  # Read-only mount
    shm_size: 32gb
    ipc: host
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    environment:
      - CUDA_VISIBLE_DEVICES=0  # Single GPU
    command: >
      python3 -m sglang.launch_server
      --model-path openai/gpt-oss-120b
      --speculative-algorithm EAGLE3
      --speculative-draft-model-path lmsys/EAGLE3-gpt-oss-120b-bf16
      --speculative-num-steps 3
      --speculative-eagle-topk 1
      --speculative-num-draft-tokens 4
      --quantization mxfp4
      --tp 1
      --max-model-len 8192
      --max-batch-size 8
      --host 0.0.0.0
      --port 30000
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:30000/v1/models"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s  # Long startup for model loading
    networks:
      - trollama-network

  # UPDATED: FastAPI service depends on SGLang
  fastapi-service:
    # ... existing config ...
    depends_on:
      sglang-server:
        condition: service_healthy  # Wait for SGLang to be ready
      dynamodb-local:
        condition: service_started
      logging-service:
        condition: service_started
    environment:
      - SGLANG_ENDPOINT=http://sglang-server:30000  # â† ADD THIS

  # EXISTING: Other services remain unchanged
  ollama:
    # ... existing config (unchanged) ...
  
  discord-bot:
    # ... existing config (unchanged) ...

networks:
  trollama-network:
    driver: bridge
```

---

## Phase 6: Model Download Script

**File**: `scripts/download_eagle3_models.sh`

```bash
#!/bin/bash
# Download gpt-oss-120b and Eagle3 models

set -e

MODELS_DIR="${MODELS_DIR:-/models}"

echo "ðŸ“¥ Downloading gpt-oss-120b base model..."
huggingface-cli download openai/gpt-oss-120b \
  --local-dir "$MODELS_DIR/gpt-oss-120b" \
  --local-dir-use-symlinks False

echo "ðŸ“¥ Downloading EAGLE3 draft model..."
huggingface-cli download lmsys/EAGLE3-gpt-oss-120b-bf16 \
  --local-dir "$MODELS_DIR/EAGLE3-gpt-oss-120b-bf16" \
  --local-dir-use-symlinks False

echo "âœ… Models downloaded to $MODELS_DIR"
echo ""
echo "Total disk usage:"
du -sh "$MODELS_DIR/gpt-oss-120b"
du -sh "$MODELS_DIR/EAGLE3-gpt-oss-120b-bf16"
```

**Usage**:
```bash
chmod +x scripts/download_eagle3_models.sh
./scripts/download_eagle3_models.sh
```

---

## Phase 7: Environment Configuration

**File**: `.env` (add these variables)

```bash
# SGLang Backend Configuration
SGLANG_ENDPOINT=http://sglang-server:30000  # Docker network
# or http://localhost:30000 if running outside Docker

# VRAM Profile (use performance for DGX Spark with Eagle3)
VRAM_PROFILE=performance

# Eagle3 Model Configuration
EAGLE3_MODEL_ID=gpt-oss-120b-eagle3
EAGLE3_VRAM_GB=84.0  # Actual measured with 40K context

# Circuit Breaker Settings
VRAM_CIRCUIT_BREAKER_ENABLED=true
VRAM_CIRCUIT_BREAKER_THRESHOLD=2
VRAM_CIRCUIT_BREAKER_WINDOW_SECONDS=300
VRAM_CIRCUIT_BREAKER_BUFFER_GB=20.0
```

---

## Phase 8: Route Assignment Updates

**File**: `fastapi-service/app/routing/route.py`

```python
# Update route definitions to use Eagle3 model

from enum import Enum

class Route(Enum):
    MATH = "MATH"
    SIMPLE_CODE = "SIMPLE_CODE"
    COMPLEX_CODE = "COMPLEX_CODE"
    REASONING = "REASONING"
    RESEARCH = "RESEARCH"
    SELF_HANDLE = "SELF_HANDLE"


# Route configurations
ROUTE_CONFIGS = {
    Route.MATH: {
        "model": "rnj-1:8b",
        "backend": "ollama",
        "temperature": 0.2,
        "tools": []
    },
    Route.SIMPLE_CODE: {
        "model": "rnj-1:8b",
        "backend": "ollama",
        "temperature": 0.2,
        "tools": []
    },
    Route.COMPLEX_CODE: {
        "model": "gpt-oss-120b-eagle3",  # â† Use Eagle3
        "backend": "sglang",              # â† SGLang backend
        "temperature": 0.3,
        "tools": []
    },
    Route.REASONING: {
        "model": "gpt-oss-120b-eagle3",  # â† Use Eagle3
        "backend": "sglang",              # â† SGLang backend
        "temperature": 0.7,
        "tools": ["web_search", "fetch_webpage"]  # Tools via Strands
    },
    Route.RESEARCH: {
        "model": "gpt-oss-120b-eagle3",  # â† Use Eagle3
        "backend": "sglang",              # â† SGLang backend
        "temperature": 0.7,
        "tools": ["web_search", "fetch_webpage"]
    },
    Route.SELF_HANDLE: {
        "model": "gpt-oss:20b",
        "backend": "ollama",
        "temperature": 0.7,
        "tools": ["web_search", "fetch_webpage"]
    }
}
```

---

## Phase 9: Discord Bot UX (No Changes Needed)

The existing Discord bot UX for handling 15-30s TTFT works perfectly with SGLang:

```python
# bot/message_handler.py - EXISTING CODE WORKS AS-IS

async def handle_user_message(message):
    # Immediate acknowledgment
    await message.add_reaction("ðŸ¤”")
    
    # Send to FastAPI via WebSocket
    await ws.send_json({
        "type": "user_message",
        "user_id": str(message.author.id),
        "thread_id": str(message.channel.id),
        "message": message.content
    })
    
    # Progressive updates during 15-30s TTFT
    response_text = ""
    last_update = time.time()
    
    async for chunk in ws.receive_stream():
        if chunk["type"] == "response_chunk":
            response_text += chunk["content"]
            
            # Update every 2 seconds during generation
            if time.time() - last_update > 2.0:
                await bot_message.edit(content=response_text[:1900])
                last_update = time.time()
        
        elif chunk["type"] == "response_complete":
            await message.clear_reaction("ðŸ¤”")
            await message.add_reaction("âœ…")
            break
```

**UX Flow remains identical**:
1. User sends message
2. Bot adds ðŸ¤” reaction (immediate)
3. 15-30s TTFT (prefill on GB10)
4. Progressive updates every 2s during generation
5. Final response + âœ… reaction

---

## Phase 10: Testing & Validation

### 1. Test SGLang Server Startup

```bash
# Start SGLang server
docker-compose up -d sglang-server

# Check logs
docker logs -f trollama-sglang

# Expected output:
# âœ… Models loaded successfully
# âœ… Server started on http://0.0.0.0:30000

# Verify models endpoint
curl http://localhost:30000/v1/models | jq
```

### 2. Test Backend Manager

```python
# test_sglang_backend.py

import asyncio
from app.implementations.sglang_backend import SGLangBackendManager

async def test_sglang():
    backend = SGLangBackendManager("http://localhost:30000")
    
    # Test health check
    health = await backend.health_check()
    assert health, "SGLang server not healthy"
    
    # Test model verification
    await backend.load("gpt-oss-120b", backend_config)
    
    # Test streaming
    prompt = "Explain quantum computing in 2 sentences."
    chunks = []
    async for chunk in backend.generate_stream(
        model_id="gpt-oss-120b",
        prompt=prompt,
        temperature=0.7,
        max_tokens=100
    ):
        chunks.append(chunk)
        print(chunk, end="", flush=True)
    
    assert len(chunks) > 0, "No chunks received"
    print(f"\nâœ… Received {len(chunks)} chunks")

asyncio.run(test_sglang())
```

### 3. Test Route Assignment

```bash
# Send test message via Discord bot
# Should route to gpt-oss-120b-eagle3 for complex queries

# Check FastAPI logs for routing decision:
# âœ… Route: COMPLEX_CODE
# âœ… Model: gpt-oss-120b-eagle3
# âœ… Backend: sglang
# âœ… VRAM available: 95GB (sufficient for 90GB requirement)
```

### 4. Test TTFT & Throughput

```bash
# Expected performance on GB10:
# - TTFT (8K context): 15-30 seconds (memory-bound)
# - Decode speed: 55-70 tok/s (with Eagle3)
# - Concurrent (4 requests): ~180-200 tok/s aggregate
```

---

## Deployment Checklist

### Pre-Deployment

- [ ] Download gpt-oss-120b + Eagle3 models (`./scripts/download_eagle3_models.sh`)
- [ ] Verify 128GB unified memory accessible on DGX Spark
- [x] Update `.env` with `SGLANG_ENDPOINT` and `VRAM_PROFILE=performance`
- [ ] Update `aggressive.py` profile: `vram_size_gb=90.0` (not 65GB)

### Deployment Steps

```bash
# 1. Pull latest SGLang image
docker pull lmsysorg/sglang:spark

# 2. Start SGLang server first (model loading takes ~5-10 minutes)
docker-compose up -d sglang-server

# 3. Monitor startup
docker logs -f trollama-sglang
# Wait for: "âœ… Server started on http://0.0.0.0:30000"

# 4. Verify models loaded
curl http://localhost:30000/v1/models | jq

# 5. Start FastAPI service (depends on SGLang)
docker-compose up -d fastapi-service

# 6. Start Discord bot
docker-compose up -d discord-bot

# 7. Verify all services healthy
docker-compose ps
curl http://localhost:8001/vram/status | jq
```

### Post-Deployment Monitoring

```bash
# Monitor VRAM usage
curl http://localhost:8001/vram/status | jq '.memory'

# Expected:
# {
#   "total_gb": 128.0,
#   "used_gb": 95.0,
#   "available_gb": 33.0,
#   "model_usage_gb": 90.0,  # gpt-oss-120b-eagle3
#   "usage_pct": 74.2
# }

# Monitor PSI (should stay below 10%)
curl http://localhost:8001/vram/psi | jq

# Monitor circuit breaker status
curl http://localhost:8001/vram/admin/crashes | jq
```

---

## Troubleshooting

### Issue: SGLang server fails to start

**Symptoms**: `docker logs trollama-sglang` shows CUDA errors or OOM

**Fix**:
```bash
# Check available memory
free -h

# Reduce max_batch_size
# In docker-compose.yml, change:
--max-batch-size 4  # Down from 8

# Or reduce max_model_len
--max-model-len 4096  # Down from 8192
```

### Issue: "Model not loaded" error from SGLang backend

**Symptoms**: `ValueError: Model 'gpt-oss-120b-eagle3' not loaded`

**Fix**:
```bash
# Check what model SGLang loaded
curl http://localhost:30000/v1/models | jq '.data[].id'

# Update model name in aggressive.py to match
# If SGLang reports "gpt-oss-120b", use that exact name
```

### Issue: Circuit breaker opens frequently

**Symptoms**: Repeated "Circuit breaker triggered" logs

**Fix**:
```bash
# Check crash history
curl http://localhost:8001/vram/admin/crashes | jq

# Clear crash history if false positives
curl -X DELETE http://localhost:8001/vram/admin/crashes/gpt-oss-120b-eagle3

# Increase failure threshold in .env
VRAM_CIRCUIT_BREAKER_THRESHOLD=5  # Up from 2
```

### Issue: Slow TTFT (>60 seconds)

**Symptoms**: First token takes over 1 minute

**Causes**:
- Context too long (>8K tokens)
- Concurrent requests saturating memory bandwidth
- PSI pressure causing swapping

**Fix**:
```bash
# Check PSI
curl http://localhost:8001/vram/psi | jq

# If psi_full_avg10 > 10%:
# 1. Reduce concurrent batch size
--max-batch-size 2

# 2. Limit context length
--max-model-len 4096

# 3. Check for memory leaks
docker stats trollama-sglang
```

---

## Performance Expectations

| Metric | Expected Value | Notes |
|--------|----------------|-------|
| **TTFT (4K context)** | 10-20s | Memory-bound on GB10 |
| **TTFT (8K context)** | 15-30s | Acceptable for Discord UX |
| **Decode speed (single)** | 55-70 tok/s | 1.6-1.8Ã— improvement with Eagle3 |
| **Decode speed (4 concurrent)** | 180-200 tok/s | Aggregate throughput |
| **Memory usage** | 85-95GB | 90GB model + 5GB overhead |
| **PSI full_avg10** | <5% | Healthy, no pressure |

---

## Key Differences from Original Plan

| Original (TensorRT-LLM) | Updated (SGLang) |
|------------------------|------------------|
| âŒ Infinite loop bug on SM 12.1 | âœ… Works on GB10 |
| âŒ Direct TensorRT-LLM API | âœ… OpenAI-compatible API |
| âŒ Dynamic model loading | âœ… Load at startup (simpler) |
| âŒ SM-specific kernels | âœ… PyTorch backend (portable) |
| âš ï¸ Untested on GB10 | âœ… Tested on Blackwell |
| ðŸ“¦ Complex integration | ðŸ“¦ Clean backend pattern |

---

## Summary

**What Changed**:
1. **Backend**: SGLang instead of TensorRT-LLM (avoids infinite loop bug)
2. **API**: OpenAI-compatible instead of TensorRT native
3. **Deployment**: Separate SGLang container, not embedded
4. **Memory**: 90GB estimate (corrected from 65GB)

**What Stayed the Same**:
1. âœ… AWS Strands orchestration (no changes)
2. âœ… VRAM orchestrator (backend-agnostic)
3. âœ… Discord bot UX (handles TTFT gracefully)
4. âœ… Route assignment (just different backend)
5. âœ… Circuit breaker pattern (works with any backend)

**Result**: Clean, maintainable integration that leverages existing architecture while avoiding TensorRT-LLM's Blackwell bugs.